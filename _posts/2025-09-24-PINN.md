# Physics-Informed Neural Networks (PINNs) and Their Structure

## Introduction to PINNs

Physics-Informed Neural Networks (PINNs) are a class of deep learning methods that seamlessly integrate the knowledge of physical laws, typically expressed as differential equations, into the training of neural networks. PINNs provide a mesh-free and data-efficient framework to solve forward and inverse problems governed by Partial Differential Equations (PDEs) or Ordinary Differential Equations (ODEs). By embedding physics into the learning architecture, PINNs enable solutions that are consistent with both observed data and underlying scientific principles.

## Mathematical Foundation

Consider a physical problem described by a general nonlinear differential operator $$\(\mathcal{N}\)$$ applied to an unknown function $$\(u(\mathbf{x}, t)\)$$:

$$
\mathcal{N}[u(\mathbf{x}, t)] = 0, \quad (\mathbf{x}, t) \in \Omega \times [0,T],
$$

where $$\(\Omega \subset \mathbb{R}^d\)$$ is the spatial domain and $$\(t\)$$ represents time.  
The problem is usually accompanied by initial and boundary conditions:

$$
\begin{cases}
\mathcal{B}[u(\mathbf{x}, t)] = g(\mathbf{x}, t), & (\mathbf{x}, t) \in \partial \Omega \times [0,T], \\
u(\mathbf{x}, 0) = u_0(\mathbf{x}), & \mathbf{x} \in \Omega,
\end{cases}
$$

where $$\(\mathcal{B}\)$$ is a boundary operator and $$\(g, u_0\)$$ are known boundary and initial data, respectively.

The goal of PINNs is to approximate $$\(u(\mathbf{x}, t)\)$$ by a neural network $$\(\hat{u}(\mathbf{x}, t; \theta)\)$$ parameterized by weights and biases $$\(\theta\)$$.

---

## PINN Architecture

- **Input Layer:**  
  Receives spatial coordinates and time points $$\((\mathbf{x}, t)\)$$.

- **Hidden Layers:**  
  Several fully connected layers with smooth nonlinear activation functions (e.g., hyperbolic tangent $$\(\tanh\)$$, sine, ReLU). These layers allow the network to model complex nonlinear behaviors inherent in physical phenomena.

- **Output Layer:**  
  Predicts the scalar or vector-valued solution $$\(\hat{u}(\mathbf{x}, t)\)$$.

---

## Incorporating Physics via Loss Functions

A key component differentiating PINNs from traditional neural networks is their loss function, which explicitly enforces the governing physics by minimizing the PDE residuals.

### PDE Residual Loss

Using automatic differentiation, the derivatives required by $$\(\mathcal{N}\)$$ are computed exactly on the neural network output $$\(\hat{u}\)$$. The PDE residual is:

$$
f(\mathbf{x}, t; \theta) = \mathcal{N}[\hat{u}(\mathbf{x}, t; \theta)].
$$

The physics loss term is the mean squared error of these residuals evaluated at a set of collocation points $$\(\{(\mathbf{x}_i, t_i)\}_{i=1}^{N_f}\)$$:

$$
L_{\text{Physics}}(\theta) = \frac{1}{N_f} \sum_{i=1}^{N_f} \left| f(\mathbf{x}_i, t_i; \theta) \right|^2.
$$

### Data Loss

If measurements or exact solution values $$\(\{u_j\}\)$$ at points:

$$
{(\mathbf{x}_j, t_j)}_{j=1}^{N_d}
$$

 are available, a supervised loss term is added:

$$
L_{\text{Data}}(\theta) = \frac{1}{N_d} \sum_{j=1}^{N_d} \left| \hat{u}(\mathbf{x}_j, t_j; \theta) - u_j \right|^2.
$$

### Boundary and Initial Condition Loss

To ensure physical consistency at the domain boundaries and initial time, corresponding loss terms are included:

$$
L_{\text{Boundary}}(\theta) = \frac{1}{N_b} \sum_{k=1}^{N_b} \left| \mathcal{B}[\hat{u}(\mathbf{x}_k, t_k; \theta)] - g(\mathbf{x}_k, t_k) \right|^2,
$$

$$
L_{\text{Initial}}(\theta) = \frac{1}{N_0} \sum_{m=1}^{N_0} \left| \hat{u}(\mathbf{x}_m, 0; \theta) - u_0(\mathbf{x}_m) \right|^2.
$$

### Combined Loss Function

The total loss minimized during training is the weighted sum:

$$
L(\theta) = \lambda_f L_{\text{Physics}}(\theta) + \lambda_d L_{\text{Data}}(\theta) + \lambda_b L_{\text{Boundary}}(\theta) + \lambda_0 L_{\text{Initial}}(\theta),
$$

where $$\(\lambda_{\cdot}\)$$ denote user-defined weights balancing the contribution of each term.

---

## Theoretical Intuition and Convergence

Under regularity assumptions on $$\(u\)$$ and $$\(\mathcal{N}\)$$, the PINN framework ensures consistency because minimizing the residual loss enforces the network to approximate $$\(u\)$$ solving the PDE. Automatic differentiation yields exact gradient computations of the network output, enabling precise evaluation of derivatives without finite difference approximations. This leads to mesh-free, high-accuracy approximations with convergence properties linked to universal approximation theorems for neural networks and the consistency of PDE residual minimization.

---

## Illustrative Case Study: 1D Heat Equation

Consider the heat equation describing heat diffusion in a one-dimensional rod:

$$
\frac{\partial u}{\partial t} = \alpha \frac{\partial^2 u}{\partial x^2}, \quad x \in [0,L], \ t \in [0,T],
$$

with thermal diffusivity constant $$\(\alpha > 0\)$$.

- **Physics Residual:**

$$
f(x,t; \theta) = \frac{\partial \hat{u}}{\partial t}(x,t;\theta) - \alpha \frac{\partial^2 \hat{u}}{\partial x^2}(x,t;\theta).
$$

- **Boundary and Initial conditions** must also be specified, e.g., Dirichlet boundaries:

$$
u(0,t) = u_L(t), \quad u(L,t) = u_R(t), \quad u(x,0) = u_0(x).
$$

The PINN seeks $$\(\hat{u}(x,t;\theta)\)$$ that minimizes the combined loss from these components.
